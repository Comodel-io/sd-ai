# Evals
CLI (command line interface) app that runs within sd-ai project to measure outputs of an engine against various quality benchmarks.

# Goals
- provide a single executable source of truth that answers the question: how good is this sd-ai engine?
- translate benchmarks outlined by [BEAMS](https://www.buffalo.edu/ai-data-science/research/projects.host.html/content/shared/www/ai-data-science/research-projects/BEAMS-Initiative.detail.html) into executable code
- allow engine creators to rapidly retest with detailed feedback (see `careful.json`)
- provide summary outputs that are useful in a variety of contexts (e.g. industry papers, leaderboards, regression tests, etc)

# Getting Started
- execute evals with `npm run evals -- -e evals/experiments/careful.json`, this will do 2 things:
    - allow you to first review the engines, tests and configuration that evals intends to run
    - execute the tests, creating several results files in top-level project directory 
- everything is configured in the required (`-e`) experiment json file defined in `evals/experiments/`
- an `experiment.json` runs all specified test categories against all specified engine configurations
- by default two experiments are included but feel free to add your own too
    - `leaderboard.json` runs every engine against every test
        - **warning** this can be quite expensive, $50+ for a single execution
    - `careful.json` is an example of using evals for testing and development
        - `verbose: true` removes the progress bar in favor of detailed output
        - `sequential: true` runs only a single test at a time
        - `categoryName: [ "specificGroupsOnly" ]` specifies specific groups rather than run all tests in a category

# Evals Structure
- all tests are located in the `categories/` folder and have 3 layers:
    - **category**: high-level capability we want to measure e.g. bias, causal reasoning, bias, safety, etc
    - **group**: a collection of tests
    - **test**: specific definition with expected result and breakdown of failures
- all `categories/$category.js` files must export: 
    - a `groups` object that maps group names to a list of tests
    - an `evaluate` function takes in the relationships generated by a running a test compares those against the `expectations` for the test
        - it returns a simple list of `failure` objects where each failure must have a `type` and can optionally include details as well 
- all `tests` must have a `name` `prompt` and `expectations` object
    - `additionalParameters` can be specified if, for example, you have a `problemStatement` you wish to provide to the engine's generate
    - `expectations` is an object you can put anything in, we just hold on to it then pass it back to your `evaluate` function later

# Managing Rate Limits 
- lots of engines run via 3rd-party llm providers who are agressive about rate limiting
- so we allow 3 parameters in `limits` for slowing the execution of evals for a given engine config:
    - `tokensPerMinute` total number of tokens (input + output + reasoning) across all requests to an engine config in a 1 minute period
    - `requestsPerMinute` number of times your engine's `generate` will be called in a 1 minute period
    - `baselineTokenUsage` a fixed constant to be added to the total number of tokens in the (prompt + current model + additional parameters)
        - this is used to account for the fact that we can't reliably determine at test time how many
            - additional input tokens are gonna be used by a specific engine
            - how many output tokens will come back
            - how many reasoning tokens the llm will wanna use
- default values should be safe for default/advanced engine using openai, teir 1, non-reasoning models anything else and you should consider "Configuration Recommendations" below

## Configuration Recommendations 
- limits specified in `leaderboard.json` experiment should be a good starting place to see what you might need to add for a given llm provider
- `tokensPerMinute` and `requestsPerMinute` are easy to find from
    - https://platform.openai.com/settings/organization/limits
    - https://ai.google.dev/gemini-api/docs/rate-limits
- `baselineTokenUsage` requires some science though
    - we recommend adding a line to your engine's generate function that prints the total number of tokens used and comparing that to
    - the "Starting test:" token count printed in `verbose: true` mode
 
## Pitfalls
- limits are implemented at engine config level not the `underlyingModel` level
    - 3 engine configs pointing to same `underlyingModel` is 3x more requests to a specific llm endpoint than expected
- limits aren't maintained between evals runs. so you can accidenilty get into trouble by stoping and starting evals a bunch within a minute
- limits don't manage requests or tokens per day which is another common rate limiting strategy implemented by llm vendors
- limits don't currently implement
    - ability have an engine make multiple llm calls per generate safely
    - a way to specify prompt and background are used included more once in calls made to llm provider
    - inclusion of currentModel
