# Evals
CLI (command line interface) app that runs within sd-ai project to measure outputs of an engine against various quality benchmarks.

# Goals
- provide a single executable source of truth that answers the question: how good is this sd-ai engine?
- translate benchmarks outlined by [BEAMS](https://www.buffalo.edu/ai-data-science/research/projects.host.html/content/shared/www/ai-data-science/research-projects/BEAMS-Initiative.detail.html) into executable code
- allow engine creators to rapidly retest with detailed feedback (see `careful.json`)
- provide summary outputs that are useful in a variety of contexts (e.g. industry papers, leaderboards, regression tests, etc)

# Getting Started
- execute evals with `npm run evals -- -e evals/experiments/careful.json`, this will do 2 things:
    - allow you to first review the engines, tests and configuration that evals intends to run
    - execute the tests, creating several results files in top-level project directory 
- everything is configured in the required (`-e`) experiment json file defined in `evals/experiments/`
- an `experiment.json` runs all specified test categories against all specified engine configurations
- by default two experiments are included but feel free to add your own too
    - `leaderboardCLD.json` and `leaderboardSFD.json` run every applicable engine (often with several underlying LLMs) against every test
        - **warning** this can be quite expensive, $50+ for a single execution
    - `careful.json` is an example of using evals for testing and development
        - `verbose: true` removes the progress bar in favor of detailed output
        - `sequential: true` runs only a single test at a time
        - `categoryName: [ "specificGroupsOnly" ]` specifies specific groups rather than run all tests in a category

# Evals Structure
- all tests are located in the `categories/` folder and have 3 layers:
    - **category**: high-level capability we want to measure e.g. bias, causal reasoning, bias, safety, etc
    - **group**: a collection of tests
    - **test**: specific definition with expected result and breakdown of failures
- all `categories/$category.js` files must export: 
    - a `groups` object that maps group names to a list of tests
    - an `evaluate` function that takes in the relationships generated by running a test and compares those against the `expectations` for the test
        - it returns a simple list of `failure` objects where each failure must have a `type` and can optionally include details as well 
- all `tests` must have a `name`, `prompt`, and `expectations` object:
    - `additionalParameters` can be specified if, for example, you have a `problemStatement` you wish to provide to the engine's generate function
    - `expectations` is an object you can put anything in we just hold on to it then pass it back to your `evaluate` function later

# Caching Engine Responses
- evals can get expensive, run slowly, and crash unexpectedly, so we cache each test result to prevent data loss if you `ctrl+c` or encounter an error
- the cache is a simple local `_in_progress.jsonl` file
    - JSONL is a format where each line is a JSON object, unlike traditional JSON where the entire file must be a valid JSON object or list

## Reusing Full Results in New Experiment Runs
- the main trick is to convert the `full_results.json` into an `in_progress.jsonl`
- this allows you to reuse already completed tests from experiments and create new combinations without fully rerunning the tests. Examples include:
    - appending to previous results, such as adding a new test category to a leaderboard without rerunning old tests
    - merging several methodically generated `careful` results into a larger result set
    - filtering down an experiment that ran many parameter combinations or LLMs to just the best or most interesting results
- the easiest method is with the CLI JSON processing tool [JQ](https://jqlang.org/):
    - `jq -c '.results[]' docs/leaderboardCLD_full_results.json > 123_careful_in_progress.jsonl`
        - Replace `careful` with the name of the experiment you plan to run.
        - Replace `123` with any 3-character experiment ID of your choice.


# Managing Rate Limits 
- many engines run via 3rd-party LLM providers who are aggressive about rate limiting
- we allow 3 parameters in `limits` for slowing the execution of evals for a given engine config:
    - `tokensPerMinute`: total number of tokens (input + output + reasoning) across all requests to an engine config in a 1 minute period
    - `requestsPerMinute`: number of times your engine's `generate` will be called in a 1 minute period
    - `baselineTokenUsage`: a fixed constant to be added to the total number of tokens in the (prompt + current model + additional parameters)
        - this is used to account for the fact that we can't reliably determine at test time:
            - How many additional input tokens will be used by a specific engine
            - How many output tokens will come back
            - How many reasoning tokens the LLM will use
- default values should be safe for default/advanced engine using OpenAI Tier 1 non-reasoning models. For anything else, consider "Configuration Recommendations" below.

## Configuration Recommendations 
- limits specified in `leaderboardCLD.json` experiment should be a good starting place to see what you might need to add for a given LLM provider
- `tokensPerMinute` and `requestsPerMinute` are easy to find from:
    - https://platform.openai.com/settings/organization/limits
    - https://ai.google.dev/gemini-api/docs/rate-limits
- `baselineTokenUsage` requires some science though
    - we recommend adding a line to your engine's generate function that prints the total number of tokens used and comparing that to
    - the "Starting test:" token count printed in `verbose: true` mode
 
## Pitfalls
- limits are implemented at engine config level, not the `underlyingModel` level:
    - 3 engine configs pointing to same `underlyingModel` is 3x more requests to a specific LLM endpoint than expected
- limits aren't maintained between evals runs, so you can accidentally get into trouble by stopping and starting evals multiple times within a minute
- limits don't manage requests or tokens per day, which is another common rate limiting strategy implemented by LLM vendors
- limits don't currently implement:
    - ability to have an engine make multiple LLM calls per generate safely
    - a way to specify if prompt and background are used/included more than once in calls made to LLM provider
    - inclusion of currentModel
