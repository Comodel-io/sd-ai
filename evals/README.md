# Evals
CLI (command line interface) app that runs within sd-ai project to measure outputs of an engine against various quality benchmarks.

# Goals
- provide a single executable source of truth that answers the question: how good is this sd-ai engine?
- translate benchmarks outlined by [BEAMS](https://www.buffalo.edu/ai-data-science/research/projects.host.html/content/shared/www/ai-data-science/research-projects/BEAMS-Initiative.detail.html) into executable code
- allow engine creators to rapidly retest with detailed feedback (see `careful.json`)
- provide summary outputs that are useful in a variety of contexts (e.g. industry papers, leaderboards, regression tests, etc)

# Getting Started
- execute evals with `npm run evals -- -e evals/experiments/careful.json`, this will do 2 things:
    - allow you to first review the engines, tests and configuration that evals intends to run
    - execute the tests, creating several results files in top-level project directory 
- everything is configured in the required (`-e`) experiment json file
- by default two experiments are included by you're welcome to add your own
    - `evals/experiments/leaderboard.json` runs every engine against every test
        - **warning** this can be quite expensive, $50+ for a single execution
    - `evals/experiments/careful.json`, is an example of using evals for testing and development
        - `verbose: true`: removes the progress bar in favor of detailed output
        - `sequential: true`: runs only a single test at a time

# Evals Structure
- all tests are located in the `categories` folder and have 3 layers:
    - **category**: high-level capability we want to measure e.g. bias, causal reasoning, bias, safety, etc
    - **group**: a collection of tests
    - **test**: specific definition with expected result and breakdown of failures
- the `categories` value in `experiment.json ` files specifies which tests are run
    - the key references a specific filename within `evals/categories/` that defines the tests that will run
    - `categoryName: true` will run all groups for that category or you can run just the groups you want with `categoryName: [ "specificGroupsOnly" ]`
- all `categories/$category.js` files must export a: 
    - `groups` object that maps group names to a list of tests
    - `evaluate` function takes in the relationships generated by a running a test compares those against the `expectations` for the test
        - it returns a simple list of `failure` objects where each failure must have a `type` and can optionally include details as well 
- all `tests` must have a `name` `prompt` and `expectations` object
    - `additionalParameters` can be specified if, for example, you have a `problemStatement` you wish to provide to the engine's generate
    - `expectations` is an object you can put anything in, we just hold on to it then pass it back to your `evaluate` function later

# Managing Rate Limits 
- lots of engines run via 3rd-party llm providers who are agressive about rate limiting
- running each model carefully with the number of tokens response to get a sense of max tokens
- then just check the teiring limits for everything else
- defualt values intended to be safe for google and openai non-reasoning models
"tokensPerMinute (TPM)": engineTests[0].engineConfig.limits.tokensPerMinute + 
    (engineTests[0].engineConfig.limits.tokensPerMinute != TOKENS_PER_MINUTE ? "*" : ""),
"requestsPerMinute (RPM)": engineTests[0].engineConfig.limits.requestsPerMinute + 
    (engineTests[0].engineConfig.limits.requestsPerMinute != REQUESTS_PER_MINUTE ? "*" : ""),
"baselineTokenUsage": engineTests[0].engineConfig.limits.baselineTokenUsage + 
    (engineTests[0].engineConfig.limits.baselineTokenUsage != BASELINE_TOKEN_USAGE ? "*" : ""),

## Tips
- default limits in `leaderboard.json` experiment should be pretty safe, **unless you're using a reasoning model**
    - especially on "high reasoning" mode 
- https://platform.openai.com/settings/organization/limits
- print the expected tokens
    - console.log("Our calculation for additional tokens beyond max: ", additionalTestParametersTokenCount);
- responses from the llm will tell you definitively how many tokens are used
    - `console.log(originalCompletion, underlyingModel);`
- you can patch in a warning when you get close to or exceed the rate limit
``` 
    fetch: async (url, init) => {
        // we need to clone this because we're gonna read the json response and once you do that 
        // you can't reread the response body again later for processing
        const originalResponse = await fetch(url, init);
        const response = originalResponse.clone();

        if (parseInt(response.headers.get('x-ratelimit-remaining-requests', '11')) < 100000 || 
            parseInt(response.headers.get('x-ratelimit-remaining-tokens', '4100')) < 5000) {
            console.log("getting scary close to rate limit");
            for (const [key, value] of response.headers) {
                if (key.startsWith('x-ratelimit')) {
                    console.log(`${key}: ${value}`);
                }
            }
            const j = await response.json()
            console.log("usage: ", j.usage);
            console.log("error: ", j.error);
        }
        return originalResponse;
    },
```
- when you have multiple engine configs leaning on the same underling model, usually makes sense to split tokens and requests per minute over all f them
