
# sd-ai
Proxy service to support compact prompts returning System Dynamics content.  

The intention is for this to be a free and public service hosted by isee systems and/or anyone else to do the engineering work of prompting LLM models for the purposes of generating CLDs and (eventually) quantitative SD models.  The service returns model information both as a JSON array of relationships, and XMILE.  This service is what we at isee systems are using / will use in the future to build our LLM features around.  

The prompts in /prompts/default use https://github.com/bear96/System-Dynamics-Bot as a point of departure.  

### To get started using this service....

1. npm i 
2. npm start 

We recommend VSCode using a launch.json for the Node type applications (you get a debugger, and hot-reloading)  

### How it works

The intent is to externalize all prompts, and LLM choices and make them availble as options for the end user. If the community desires we're open to supporting other APIs besides OpenAI.  This allows researchers in the field to do prompt engineering, and perfect the science around generating SD content from LLMs without having to worry about the engineering to make their work more generally available.  Stella Professional/Stella Architect or any other client which consumes this service will do the work of graph drawing, and user editing of returned models, allowing researchers within the field to focus purely on developing better ways to interact with LLMs. 

To make your own set of prompts, copy the /prompts/default folder, make a new folder with the same 4 files (different content, but same names) and make the directory name something identifiable for the end user. When you do this be sure to preserve all JSON formatting information in the system prompt and the check (polarity) prompt as that is what the OpenAIWrapper class expects to get as a reply.  By making your own folder of prompts when you restart the server the user will be presented with an option to select your prompting scheme from a dropdown list.  

The service can be run with an embedded OpenAI key (see note below) or an OpenAI key can be provided to each API call which interacts with OpenAI.  The Stella client has a place where a key can be provided, as well as a service address so that the Stella client can be pointed at any version of this service hosted by anyone else, including localhost for developers.

The service by default keeps track of the full "conversation" with the LLM internally in memory (no Redis session store yet) using express-session.  This allows you to incrementally build models through many prompts without having the client have to pass the current state of the model on every subsequent call.  You can also override this functionality (which we do in Stella) passing the current state of the model to each `generate` call.  

The service returns both a minimally viable XMILE representation of the model (no diagram information) which can be opened directly in Stella v3.7.3 or later and the view information will be machine generated by Stella, as well as a JSON array of relationship information.  This JSON array of relationship information is how state is maintained by the service.  

### API Documentation

Below are the four REST API calls this service support, written in the order they are typically made

1. GET /api/v1/query-info

Takes no arguements returns the list of models that should be usable using the OpenAI node wrapper that OpenAI makes available.  You do not need to use one of these model strings, and if you know of another model supported by the OpenAI platform feel free to pass it.  It also returns a list of promptingSchemes (described in the section above) which the end user can use to generate their model

Returns a `{success: <bool>, models: <string array of model names>, promptingSchemes: <string array referring to a sub-folder of /prompts>}`

2. GET /api/v1/initialize

This call can be skipped, but is useful for determining if your client is supported by the service.  

This call takes 5 optional query parameters

`clientProduct` - String - The product name of the client that is talking with the service (for support checks) (e.g. Stella Architect).  
`clientVersion` - String - The version number of the client that is talking with the service (for support checks) (e.g. 3.7.3).  
`openAIKey` - String - A key that overrides the default on the server (see final section).  If set here, it will be used for all subsequent calls unless otherwise changed.  
`openAIModel` - String - The name of the model to use for model generation. If set here, it will be used for all subsequent calls unless otherwise changed.  
`promptSchemeId` - String - One of the prompt scheme id's from the query-info call. If set here, it will be used for all subsequent calls unless otherwise changed.  

Returns `{success: <bool>, message: <string> }`

3. POST /api/v1/generate

This call is the work-horse of the service, doing the job of diagram generation.

This call takes 1 required post parameter

`userPrompt` - The prompt typed in by the end user

This call takes 4 optional post parameters  
`openAIKey` - String - A key that overrides the default on the server (see final section).  If set here, it will be used for all subsequent calls unless otherwise changed.  
`openAIModel` - String - The name of the model to use for model generation. If set here, it will be used for all subsequent calls unless otherwise changed.  
`promptSchemeId` - String - One of the prompt scheme id's from the query-info call. If set here, it will be used for all subsequent calls unless otherwise changed.  
`lastRelationshipList` - JSON String - A serialized string of relationships in the same format as the result to feed back into the LLM.  If not provided, the last call to `generate`'s response is used.  

Returns `{success: <bool>, message: <string>, err: <error object>, xmile: <string>, relationships: <array of relationships>}`  

All relationships in the entire diagram will be returned as both XMILE or in the relationship array irregardless of whether they are new or not.  The client is expected to do any diff/update operations if desired.  The "normal" usecase is that each call to generate returns a whole "new" model.

Relationship format is:
`{"reasoning": "<string>", "causal relationship": "source --> destination",  "relevant text": "<string>", "polarity": <string ?, +, ->, "polarity reasoning": <string> }`  

4. GET /api/v1/finalize

This destroys the active session.  

Returns `{success: <bool>, message: <string> }`  

### Important Note 
You must have a .env file at the top level which can have the following keys  
 * OPEN_API_KEY which is your open AI access token, if provided then clients do not need to provide one to either the intialize or generate calls  
 * SESSION_SECRET which is a string used by express-session to secure the cookies this is required
